{
  "provider": "ollama",
  "endpoint": "http://localhost:11434",
  "default_model": "llama2:latest",
  "available_models": [
    {
      "id": "llama2:latest",
      "name": "Llama 2",
      "variants": ["7b", "13b", "70b"],
      "context_window": 4096,
      "strengths": ["general purpose", "open source", "privacy"],
      "gpu_layers": 35
    },
    {
      "id": "codellama:latest",
      "name": "Code Llama",
      "variants": ["7b", "13b", "34b"],
      "context_window": 16384,
      "strengths": ["code generation", "debugging", "explanation"],
      "gpu_layers": 35
    },
    {
      "id": "mistral:latest",
      "name": "Mistral",
      "variants": ["7b"],
      "context_window": 8192,
      "strengths": ["efficiency", "instruction following"],
      "gpu_layers": 35
    },
    {
      "id": "mixtral:latest",
      "name": "Mixtral",
      "variants": ["8x7b"],
      "context_window": 32768,
      "strengths": ["MoE architecture", "performance"],
      "gpu_layers": 35
    },
    {
      "id": "phi-2:latest",
      "name": "Phi-2",
      "variants": ["2.7b"],
      "context_window": 2048,
      "strengths": ["small size", "efficiency", "reasoning"],
      "gpu_layers": 35
    }
  ],
  "default_parameters": {
    "temperature": 0.7,
    "top_k": 40,
    "top_p": 0.9,
    "repeat_penalty": 1.1,
    "seed": null,
    "num_ctx": 4096,
    "num_gpu": 35,
    "num_thread": 8
  },
  "pull_settings": {
    "auto_pull": true,
    "show_progress": true,
    "insecure": false
  },
  "model_paths": {
    "models": "~/.ollama/models",
    "custom": "~/dotfiles-ai/ai-tools/models/ollama/custom"
  },
  "performance": {
    "mmap": true,
    "mlock": false,
    "f16_kv": true,
    "low_vram": false,
    "main_gpu": 0
  },
  "timeout_seconds": 300
}